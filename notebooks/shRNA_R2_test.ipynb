{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../modules\" not in sys.path:\n",
    "    sys.path.append(\"../modules\")\n",
    "import preprocess\n",
    "from taigapy import TaigaClient\n",
    "\n",
    "import demeter2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "alt_tol = 1e-5 #convergence relative tolerance (on change in cost) for blockwise coord descent procedure\n",
    "fit_CL_slopes = True \n",
    "regions = [(10,17),(11,18)] #seed sequence regions\n",
    "data_dir = './data/'\n",
    "\n",
    "# data_files = '/data/data_files_Achilles'\n",
    "data_files = '/test/data_files_Achilles_paths'\n",
    "D2_results_dir = '/results/kube_results/Ach_final/1/'\n",
    "\n",
    "new_name_map = pd.read_csv('/results/results/name_change_map.csv')\n",
    "new_name_map_dict = {a: b for a, b in zip(new_name_map.old_name, new_name_map.new_name)}\n",
    "\n",
    "# data_files = '/test/data_files_DRIVE_paths'\n",
    "# # data_files = '/test/data_files_DRIVE'\n",
    "# # data_files = '/Users/jmmcfarl/CPDS/demeter2/data/data_files_DRIVE'\n",
    "# D2_results_dir = '/results/kube_results/DRIVE_final/1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /data/achilles-98k-repcollapsed-lfc.csv\n",
      "Loading dataset from: /data/achilles-55k-batch2-repcollapsed-lfc.csv\n",
      "Loading dataset from: /data/achilles-55k-batch1-repcollapsed-lfc.csv\n",
      "Read 3 data files\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('/taiga/token'):\n",
    "    tc = TaigaClient(token_path = '/taiga/token')\n",
    "elif os.path.exists('taiga/token'):\n",
    "    tc = TaigaClient(token_path = 'taiga/token')\n",
    "# tc = TaigaClient()\n",
    "\n",
    "input_data = pd.read_csv(data_files, sep = '\\t').dropna(axis = 0, how = 'all')\n",
    "def get_dset(input_info):\n",
    "    if len(input_info) == 1:\n",
    "        print('Loading dataset from: {}'.format(input_info['data_file_paths']))\n",
    "        cur_data = pd.read_csv(input_info['data_file_paths'], index_col = 0)\n",
    "    else:\n",
    "        print('Fetching taiga dataset: {} file: {}, version: {}'.format(input_info['data_set'], \n",
    "            input_info['data_file'], \n",
    "            int(input_info['version'])))\n",
    "        if pd.isnull(input_info['data_file']):\n",
    "            cur_data = tc.get(\n",
    "                name = input_info['data_set'],\n",
    "                version = int(input_info['version']))\n",
    "        else:\n",
    "            cur_data = tc.get(\n",
    "                name = input_info['data_set'],\n",
    "                file = input_info['data_file'],\n",
    "                version = int(input_info['version']), force = True)\n",
    "    return(cur_data)\n",
    "dsets = [get_dset(x) for ii, x in input_data.iterrows()]\n",
    "print('Read {} data files'.format(len(dsets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading positive/negative control sets\n",
      "Using 217 positive and 926 negative control genes for training\n",
      "Making processed data\n",
      "Eliminating 552 promiscuous hairpins\n",
      "Eliminated 3634/102295 non-targeting hairpins from map\n",
      "Identified 334 gene families\n",
      "Creating dataset with:\n",
      "93863 hairpins\n",
      "501 CLs\n",
      "17507 genes\n",
      "15135 seeds\n"
     ]
    }
   ],
   "source": [
    "# sh_targets = tc.get(name='gpp-shrna-mapping-8759', version=2, file='shmap_19mer_noXLOC')\n",
    "sh_targets = pd.read_csv('/data/shRNA-mapping.csv')\n",
    "sh_targets.rename(columns = {'Barcode Sequence': 'SEQ', 'Gene ID': 'Gene_ID'}, inplace=True)\n",
    "\n",
    "sh_targets.dropna(subset = ['Gene_ID'], inplace=True)\n",
    "\n",
    "#load curated pos and negative control gene sets\n",
    "print('Loading positive/negative control sets')\n",
    "#load Entrez IDs for pos and neg con genes\n",
    "# pos_con_genes = tc.get(name='demeter2-pos-neg-controls-a5c6', version=1, file='hart_pos_controls')['Gene_ID'].values\n",
    "# neg_con_genes = tc.get(name='demeter2-pos-neg-controls-a5c6', version=1, file='hart_neg_controls')['Gene_ID'].values\n",
    "pos_con_genes = pd.read_csv('/data/Hart-pos-controls.csv')['Gene_ID'].values\n",
    "neg_con_genes = pd.read_csv('/data/Hart-neg-controls.csv')['Gene_ID'].values\n",
    "\n",
    "train_neg_con_genes = neg_con_genes\n",
    "train_pos_con_genes = pos_con_genes\n",
    "test_neg_con_genes = np.setdiff1d(neg_con_genes, train_neg_con_genes)\n",
    "test_pos_con_genes = np.setdiff1d(pos_con_genes, train_pos_con_genes)\n",
    "print('Using {} positive and {} negative control genes for training'.format(len(train_pos_con_genes), \n",
    "    len(train_neg_con_genes)))\n",
    "\n",
    "#parse data\n",
    "print('Making processed data')\n",
    "data = preprocess.make_demeter2_data(dsets, sh_targets)\n",
    "\n",
    "data_names = {'genes': data['unique_genes'],\n",
    "             'CLs': data['unique_CLs'],\n",
    "             'hps': data['unique_hp_seqs'],\n",
    "             'seeds': data['unique_seed_seqs']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(demeter2)\n",
    "test_inds = None\n",
    "gene_controls = {'pos': train_pos_con_genes, 'neg': train_neg_con_genes,\n",
    "                'pos_test': test_pos_con_genes, 'neg_test': test_neg_con_genes}\n",
    "mod = demeter2.demeter(data['LFC_mats'], data['gene_matrix'], data['seed_matrix'], \n",
    "                   gene_sets = gene_controls, data_names = data_names, test_inds = test_inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_matrix(path):\n",
    "    mat = pd.read_csv(path)\n",
    "    mat.iloc[:,0] = mat.iloc[:,0].astype(str)\n",
    "    mat.set_index(mat.columns[0], inplace=True)\n",
    "    return(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_data = pd.read_csv(os.path.join(D2_results_dir, 'CL_data.csv'))\n",
    "CL_data['CCLE_ID'] = CL_data['CCLE_ID'].replace(new_name_map_dict)\n",
    "CL_data.set_index('CCLE_ID', inplace = True)\n",
    "CL_data = CL_data.ix[mod.data_names['CLs']]\n",
    "CL_data.reset_index(inplace=True)\n",
    "np.testing.assert_array_equal(mod.data_names['CLs'],CL_data['CCLE_ID'])\n",
    "_=mod.sess.run(mod.gene_slope.assign(CL_data.gene_slope.values.reshape(-1,1)))\n",
    "\n",
    "# CL_batch_data = pd.read_csv(os.path.join(D2_results_dir, 'CL_batch_data.csv')).set_index('CCLE_ID')\n",
    "CL_batch_data = pd.read_csv(os.path.join(D2_results_dir, 'CL_batch_data.csv'))\n",
    "CL_batch_data['CCLE_ID'] = CL_batch_data['CCLE_ID'].replace(new_name_map_dict)\n",
    "CL_batch_data.set_index('CCLE_ID', inplace = True)\n",
    "np.testing.assert_array_equal(mod.all_CL_names,CL_batch_data.index.values)\n",
    "_=mod.sess.run(mod.CL_slope.assign(CL_batch_data.CL_slope.values.reshape(-1,1)))\n",
    "_=mod.sess.run(mod.CL_offset.assign(CL_batch_data.CL_offset.values.reshape(-1,1)))\n",
    "_=mod.sess.run(mod.CL_noise_vars.assign(CL_batch_data.noise_vars.values.reshape(-1,1)))\n",
    "\n",
    "hp_data = pd.read_csv(os.path.join(D2_results_dir, 'hp_data.csv'))\n",
    "np.testing.assert_array_equal(mod.data_names['hps'],hp_data['hp'])\n",
    "_=mod.sess.run(mod.guide_Geff.assign(hp_data.Geff.values.reshape(-1,1)))\n",
    "_=mod.sess.run(mod.guide_Seff.assign(hp_data.Seff.values.reshape(-1,1)))\n",
    "_=mod.sess.run(mod.hairpin_unpred.assign(hp_data.unpred_offset.values.reshape(-1,1)))\n",
    "\n",
    "hp_batch_data = pd.read_csv(os.path.join(D2_results_dir, 'hp_batch_data.csv')).set_index('hp')\n",
    "np.testing.assert_array_equal(mod.all_hp_seqs,hp_batch_data.index.values)\n",
    "_=mod.sess.run(mod.hairpin_offset.assign(hp_batch_data.hairpin_offset.values.reshape(-1,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFC_mats_no_na = []\n",
    "for LFC_mat in data['LFC_mats']:\n",
    "    cur = LFC_mat.values.copy()\n",
    "    cur[np.isnan(cur)] = 0\n",
    "    LFC_mats_no_na.append(cur)\n",
    "\n",
    "feed_dict = {i: d for i, d in zip(mod.obs, LFC_mats_no_na)}\n",
    "train_eval_masks = demeter2.make_eval_masks(LFC_mats_no_na, None)\n",
    "train_mask_dict = {i: d for i, d in zip(mod.eval_mask, train_eval_masks)}\n",
    "feed_dict = demeter2.merge_dicts(feed_dict, train_mask_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_R2_df(mod, data):\n",
    "    shRNA_R2 = mod.sess.run(mod.shRNA_R2, feed_dict = feed_dict)\n",
    "    shRNA_nLL = mod.sess.run(mod.shRNA_nLL, feed_dict = feed_dict)\n",
    "    shRNA_SS = mod.sess.run(mod.shRNA_oSS, feed_dict = feed_dict)\n",
    "    ovR2 = mod.sess.run(mod.R2, feed_dict = feed_dict)\n",
    "    print(ovR2)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for ii in range(len(data['LFC_mats'])):\n",
    "        cur_df = pd.DataFrame()\n",
    "        cur_df['SSE'] = shRNA_nLL[ii] * 2 #LL are originally divided by 2\n",
    "        cur_df['SST'] = shRNA_SS[ii] * 2\n",
    "        cur_df['hp'] = data['LFC_mats'][ii].index.values\n",
    "        cur_df['n_CLs'] = data['LFC_mats'][ii].shape[1]\n",
    "        df = pd.concat([df, cur_df])\n",
    "    tot_CLs = np.sum(np.array([x.shape[1] for x in data['LFC_mats']]))\n",
    "    df['SSE'] = df['SSE'] * df['n_CLs']\n",
    "    df['SST'] = df['SST'] * df['n_CLs']\n",
    "    df = df.groupby('hp').agg({'SSE': ['sum'], 'SST': ['sum'], 'n_CLs': ['sum']})\n",
    "    df[['SSE', 'SST']] = df[['SSE', 'SST']] / tot_CLs\n",
    "\n",
    "    df['R2'] = 1 - df['SSE'] / df['SST']\n",
    "    df.columns = ['SSE', 'SST', 'n_CLs', 'R2']    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.579312097121\n"
     ]
    }
   ],
   "source": [
    "gene_means = load_matrix(os.path.join(D2_results_dir, 'gene_means.csv'))\n",
    "gene_means.columns = gene_means.columns.to_series().replace(new_name_map_dict)\n",
    "gene_means.columns.values\n",
    "gene_means = gene_means.ix[:,mod.data_names['CLs']]\n",
    "np.testing.assert_array_equal(mod.data_names['CLs'],gene_means.columns.values)\n",
    "gene_means.fillna(0, inplace=True)\n",
    "_=mod.sess.run(mod.gene_score.assign(gene_means.values.transpose()))\n",
    "\n",
    "gene_R2_df = get_R2_df(mod, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759951044973\n"
     ]
    }
   ],
   "source": [
    "seed_means = load_matrix(os.path.join(D2_results_dir, 'seed_means.csv'))\n",
    "seed_means.columns = seed_means.columns.to_series().replace(new_name_map_dict)\n",
    "seed_means.columns.values\n",
    "seed_means = seed_means.ix[:,mod.data_names['CLs']]\n",
    "np.testing.assert_array_equal(mod.data_names['CLs'],seed_means.columns.values)\n",
    "seed_means.fillna(0, inplace=True)\n",
    "_=mod.sess.run(mod.seed_score.assign(seed_means.values.transpose()))\n",
    "\n",
    "gene_means = gene_means*0\n",
    "_=mod.sess.run(mod.gene_score.assign(gene_means.values.transpose()))\n",
    "\n",
    "seed_R2_df = get_R2_df(mod, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_R2_df = pd.merge(gene_R2_df, seed_R2_df[['SSE', 'R2']], left_index = True, right_index = True, suffixes = ['_gene', '_seed'])\n",
    "comb_R2_df.to_csv(os.path.join(D2_results_dir, 'hp_R2_post.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
