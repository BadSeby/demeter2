{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from taigapy import TaigaClient\n",
    "\n",
    "#PARAMETERS\n",
    "min_avg_Geff = 0.2\n",
    "min_sum_Geff = 1.5\n",
    "min_hps_per_gene = 3\n",
    "\n",
    "c = TaigaClient()\n",
    "\n",
    "base_data_dir = '/Users/jmmcfarl/CPDS/demeter2'\n",
    "\n",
    "Ach_model_dir = os.path.join(base_data_dir,'kube_results/Ach_final/1')\n",
    "Ach_dataset_id = 'demeter2-achilles-5386'\n",
    "\n",
    "DRIVE_model_dir = os.path.join(base_data_dir, 'kube_results/DRIVE_final/1')\n",
    "DRIVE_dataset_id = 'demeter2-drive-0591'\n",
    "\n",
    "Marc_model_dir = os.path.join(base_data_dir, 'kube_results/Marc_final/1')\n",
    "Marc_dataset_id = 'demeter2-marcotte-a703'\n",
    "\n",
    "comb_model_dir = os.path.join(base_data_dir, 'kube_results/comb_final/1')\n",
    "comb_dataset_id = 'demeter2-combined-dc9c'\n",
    "\n",
    "new_name_map = pd.read_csv('/Users/jmmcfarl/CPDS/demeter2/results/name_change_map.csv')\n",
    "new_name_map_dict = {a: b for a, b in zip(new_name_map.old_name, new_name_map.new_name)}\n",
    "\n",
    "name_correction_dict = pd.read_csv('/Users/jmmcfarl/CPDS/demeter2/data/CCLE_name_corrections.csv')\n",
    "name_correction_dict = {a: b for a, b in zip(name_correction_dict.old_name, name_correction_dict.new_name)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hart_ess = c.get(name='demeter2-pos-neg-controls-a5c6', version=1, file='hart_pos_controls')['Gene_ID'].values.astype(str)\n",
    "hart_non_ess = c.get(name='demeter2-pos-neg-controls-a5c6', version=1, file='hart_neg_controls')['Gene_ID'].values.astype(str)\n",
    "\n",
    "sh_targets = c.get(name = 'gpp-shrna-mapping-8759', version = 2, file = 'shmap_19mer_noXLOC')\n",
    "sh_targets = sh_targets.rename(columns = {'Barcode Sequence': 'hp'}, inplace = False).set_index('hp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene_name_df = sh_targets.ix[~sh_targets['Gene ID'].str.contains('^NO_CURRENT')].copy()\n",
    "gene_name_df['Gene name'] = gene_name_df['Gene Symbol'] + ' (' + gene_name_df['Gene ID'] + ')'\n",
    "gene_name_map = gene_name_df.set_index('Gene ID', inplace = False)['Gene name']\n",
    "gene_name_map = gene_name_map.to_dict()\n",
    "\n",
    "gene_sym_map = gene_name_df.set_index('Gene ID', inplace = False)['Gene Symbol']\n",
    "gene_sym_map = gene_sym_map.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D2_description='''\n",
    "Results from DEMETER2 model fit.\n",
    "\n",
    "Contents:\n",
    "\n",
    "* gene_means_proc: posterior mean estimates of essentiality for each gene/CL pair\n",
    "* gene_SDs_proc: posterior SD of essentiality estimates for each gene/CL pair  \n",
    "* hp_data_comb: model results for each hairpin, including:\n",
    "    * Geff: hairpin gene knockdown efficacy [0,1]\n",
    "    * Seff: hairpin seed knockdown efficacy [0,1]\n",
    "    * unpred_offset_mean: posterior mean of across-CL avg unpredicted offtarget effect\n",
    "    * unpred_offset_sd: posterior SD of across-CL avg unpredicted offtarget effect\n",
    "* CL_data_comb: model results for each CL, including:\n",
    "    * gene_slope: RNAi efficacy parameter\n",
    "    * CL_slope: overall scaling factor\n",
    "    * noise_vars: noise variance\n",
    "    * offset_mean: posterior mean of additive offset\n",
    "    * offset_SD: posterior SD of additive offset\n",
    "    \n",
    "versions:\n",
    "* v2: \n",
    "    * Run with final hyperparameter settings\n",
    "    * Add gene symbols to entrez IDs\n",
    "    * exclude genes with all NA values\n",
    "    * exclude genes with poor reagents\n",
    "    * normalize gene scores to have median of pos-cons at -1 and median of neg-cons at 0\n",
    "\n",
    "* v3: \n",
    "    * Add gene symbols for gene families\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_matrix(path):\n",
    "    mat = pd.read_csv(path)\n",
    "    mat.iloc[:,0] = mat.iloc[:,0].astype(str)\n",
    "    mat.set_index(mat.columns[0], inplace=True)\n",
    "    return(mat)\n",
    "    \n",
    "\n",
    "def make_processed_gene_data_D2(cur_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff):\n",
    "    '''Process gene means and SDs and make new files'''\n",
    "#     gs = pd.read_csv(os.path.join(cur_model_dir, 'gene_means.csv'), index_col = 0)\n",
    "#     gs_unc = pd.read_csv(os.path.join(cur_model_dir, 'gene_SDs.csv'), index_col = 0)\n",
    "    gs = load_matrix(os.path.join(cur_model_dir, 'gene_means.csv'))\n",
    "    gs_unc = load_matrix(os.path.join(cur_model_dir, 'gene_SDs.csv'))\n",
    "\n",
    "    #update cell line names\n",
    "    gs.columns = gs.columns.to_series().replace(new_name_map_dict)\n",
    "    gs_unc.columns = gs_unc.columns.to_series().replace(new_name_map_dict)\n",
    "    \n",
    "    #fix CCLE name problems\n",
    "    gs.columns = gs.columns.to_series().replace(name_correction_dict)\n",
    "    gs_unc.columns = gs_unc.columns.to_series().replace(name_correction_dict)\n",
    "    \n",
    "    #drop non Entrez genes\n",
    "    gs = gs.filter(regex = '[0-9]', axis = 0)\n",
    "    gs_unc = gs_unc.filter(regex = '[0-9]', axis = 0)\n",
    "    \n",
    "    #drop genes which are NA for all cell lines\n",
    "    bad_genes = np.where(gs.isnull().sum(axis = 1) == gs.shape[1])[0]\n",
    "    print('Removing {} genes with all NAs'.format(len(bad_genes)))\n",
    "    gs.drop(gs.index[bad_genes], axis=0, inplace=True)\n",
    "    gs_unc.drop(gs_unc.index[bad_genes], axis=0, inplace=True)\n",
    "\n",
    "    #calc mean Geff and sum Geff per gene, and filter out bad-quality genes\n",
    "    hp_data = pd.read_csv(os.path.join(cur_model_dir, 'hp_data.csv')).set_index('hp')\n",
    "    hp_data = hp_data.join(sh_targets, how = 'left')\n",
    "    hp_stats = hp_data.groupby('Gene ID').agg({'Geff': [np.mean, np.sum, 'count']})\n",
    "    bad_genes = hp_stats.ix[(hp_stats['Geff']['mean'].values < min_avg_Geff) | (hp_stats['Geff']['sum'].values < min_sum_Geff) | \\\n",
    "                            (hp_stats['Geff']['count'].values < min_hps_per_gene)].index.values\n",
    "    bad_genes = np.intersect1d(bad_genes, gs.index.values)\n",
    "    print('Removing {} genes with all poor hp data'.format(len(bad_genes)))\n",
    "    gs.drop(bad_genes, axis=0, inplace=True)\n",
    "    gs_unc.drop(bad_genes, axis=0, inplace=True)\n",
    "\n",
    "    #normalize gene scores by pos-neg control medians\n",
    "    weights = 1/(gs_unc**2)\n",
    "    per_gene_avgs = np.sum(gs * weights, axis = 1) / np.sum(weights, axis = 1)\n",
    "    pos_con_median = np.nanmedian(per_gene_avgs.ix[hart_ess])\n",
    "    neg_con_median = np.nanmedian(per_gene_avgs.ix[hart_non_ess])\n",
    "\n",
    "    norm_gs_unc = gs_unc / (neg_con_median - pos_con_median)\n",
    "    norm_gs = (gs - neg_con_median) / (neg_con_median - pos_con_median)\n",
    "\n",
    "    #rename genes to include gene symbol\n",
    "    norm_gs.rename(index = gene_name_map, inplace = True)\n",
    "    norm_gs_unc.rename(index = gene_name_map, inplace = True)\n",
    "    \n",
    "    #handle gene names for gene families\n",
    "    gene_families = np.where(norm_gs.index.str.contains('&', na = False))[0]\n",
    "    ind_names = norm_gs.index.values\n",
    "    for fam_ind in gene_families:\n",
    "        cur_fam = norm_gs.index.values[fam_ind]\n",
    "#         print(cur_fam)\n",
    "        fam_syms = '&'.join([gene_sym_map[x] for x in re.split('&', cur_fam)])\n",
    "#         ind_names[fam_ind] = cur_fam + ' (' + fam_syms + ')'\n",
    "        ind_names[fam_ind] = fam_syms + ' (' + cur_fam + ')'\n",
    "    norm_gs.index = ind_names\n",
    "    norm_gs_unc.index = ind_names\n",
    "\n",
    "    norm_gs.to_csv(os.path.join(cur_model_dir, 'gene_means_proc.csv'))\n",
    "    norm_gs_unc.to_csv(os.path.join(cur_model_dir, 'gene_SDs_proc.csv'))\n",
    "\n",
    "    \n",
    "def prepare_D2_outputs(D2_model_dir):\n",
    "    '''Combine batch and non-batch parameters for CL data and hp_data'''\n",
    "#     CL_data = pd.read_csv(os.path.join(D2_model_dir, 'CL_data.csv'), index_col = 0)\n",
    "    CL_data = load_matrix(os.path.join(D2_model_dir, 'CL_data.csv'))\n",
    "    \n",
    "#     CL_batch_data = pd.read_csv(os.path.join(D2_model_dir, 'CL_batch_data.csv'), index_col = 0)\n",
    "    CL_batch_data = load_matrix(os.path.join(D2_model_dir, 'CL_batch_data.csv'))\n",
    "   \n",
    "    #rename cell lines\n",
    "    CL_data.index = CL_data.index.to_series().replace(new_name_map_dict)\n",
    "    CL_batch_data.index = CL_batch_data.index.to_series().replace(new_name_map_dict)\n",
    " \n",
    "    #fix CCLE name problems\n",
    "    CL_data.index = CL_data.index.to_series().replace(name_correction_dict)\n",
    "    CL_batch_data.index = CL_batch_data.index.to_series().replace(name_correction_dict)\n",
    "\n",
    "    CL_batch_data.reset_index(inplace=True)\n",
    "    CL_batch_data['offset_var'] = CL_batch_data['offset_sd']**2\n",
    "    CL_batch_means = CL_batch_data.groupby('CCLE_ID')[['CL_slope', 'noise_vars', 'offset_mean', 'offset_var']].agg('mean')\n",
    "    CL_batch_means['offset_sd'] = np.sqrt(CL_batch_means['offset_var'].values)\n",
    "\n",
    "    CL_data = pd.merge(CL_data, CL_batch_means[['CL_slope', 'noise_vars', 'offset_mean', 'offset_sd']], left_index=True, right_index = True)\n",
    "    CL_data.to_csv(os.path.join(D2_model_dir, 'CL_data_comb.csv'))\n",
    "    \n",
    "    hp_data = pd.read_csv(os.path.join(D2_model_dir, 'hp_data.csv')).set_index('hp')\n",
    "    hp_batch_data = pd.read_csv(os.path.join(D2_model_dir, 'hp_batch_data.csv')).reset_index()\n",
    "    hp_batch_data['hairpin_offset_var'] = hp_batch_data['hairpin_offset_sd']**2\n",
    "    hp_batch_means = hp_batch_data.groupby('hp')[['hairpin_offset_mean', 'hairpin_offset_var']].agg('mean')\n",
    "    hp_batch_means['hairpin_offset_sd'] = np.sqrt(hp_batch_means['hairpin_offset_var'].values)\n",
    "\n",
    "    hp_data = pd.merge(hp_data[['Geff', 'Seff', 'unpred_offset_mean', 'unpred_offset_sd']], hp_batch_means[['hairpin_offset_sd', 'hairpin_offset_mean']], left_index=True, right_index = True)\n",
    "    hp_data.to_csv(os.path.join(D2_model_dir, 'hp_data_comb.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Achilles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 387 genes with all NAs\n",
      "Removing 358 genes with all poor hp data\n"
     ]
    }
   ],
   "source": [
    "make_processed_gene_data_D2(Ach_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(Ach_model_dir)\n",
    "\n",
    "# c.update_dataset(dataset_permaname=Ach_dataset_id,\n",
    "# #     dataset_description=D2_description,\n",
    "#     upload_file_path_dict={os.path.join(Ach_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(Ach_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(Ach_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(Ach_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRIVE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 646 genes with all NAs\n",
      "Removing 571 genes with all poor hp data\n"
     ]
    }
   ],
   "source": [
    "make_processed_gene_data_D2(DRIVE_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(DRIVE_model_dir)\n",
    "\n",
    "# c.update_dataset(dataset_permaname=DRIVE_dataset_id,\n",
    "# #     dataset_description=D2_description,\n",
    "#     upload_file_path_dict={os.path.join(DRIVE_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(DRIVE_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(DRIVE_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(DRIVE_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcotte data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 582 genes with all NAs\n",
      "Removing 3031 genes with all poor hp data\n"
     ]
    }
   ],
   "source": [
    "make_processed_gene_data_D2(Marc_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(Marc_model_dir)\n",
    "\n",
    "# c.update_dataset(dataset_permaname=Marc_dataset_id,\n",
    "# #     dataset_description=D2_description,\n",
    "#     upload_file_path_dict={os.path.join(Marc_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(Marc_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(Marc_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(Marc_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 414 genes with all NAs\n",
      "Removing 895 genes with all poor hp data\n"
     ]
    }
   ],
   "source": [
    "make_processed_gene_data_D2(comb_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(comb_model_dir)\n",
    "\n",
    "# c.update_dataset(dataset_permaname=comb_dataset_id,\n",
    "# #     dataset_description=D2_description,\n",
    "#     upload_file_path_dict={os.path.join(comb_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(comb_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(comb_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "#                           os.path.join(comb_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
