{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from taigapy import TaigaClient\n",
    "\n",
    "#PARAMETERS\n",
    "min_avg_Geff = 0.2\n",
    "min_sum_Geff = 1.5\n",
    "\n",
    "c = TaigaClient()\n",
    "\n",
    "base_data_dir = '/Users/jmmcfarl/CPDS/demeter2'\n",
    "\n",
    "Ach_model_dir = os.path.join(base_data_dir,'kube_results/Ach_final/1')\n",
    "Ach_dataset_id = 'demeter2-achilles-5386'\n",
    "\n",
    "DRIVE_model_dir = os.path.join(base_data_dir, 'kube_results/DRIVE_final/1')\n",
    "DRIVE_dataset_id = 'demeter2-drive-0591'\n",
    "\n",
    "Marc_model_dir = os.path.join(base_data_dir, 'kube_results/Marc_final/1')\n",
    "Marc_dataset_id = 'demeter2-marcotte-a703'\n",
    "\n",
    "comb_model_dir = os.path.join(base_data_dir, 'kube_results/comb_final/1')\n",
    "comb_dataset_id = 'demeter2-combined-dc9c'\n",
    "\n",
    "new_name_map = pd.read_csv('/Users/jmmcfarl/CPDS/demeter2/results/name_change_map.csv')\n",
    "new_name_map_dict = {a: b for a, b in zip(new_name_map.old_name, new_name_map.new_name)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hart_ess = c.get(name='demeter2-pos-neg-controls-a5c6', version=1, file='hart_pos_controls')['Gene_ID'].values.astype(str)\n",
    "hart_non_ess = c.get(name='demeter2-pos-neg-controls-a5c6', version=1, file='hart_neg_controls')['Gene_ID'].values.astype(str)\n",
    "\n",
    "sh_targets = c.get(name = 'gpp-shrna-mapping-8759', version = 1, file = 'CP1175_20171102_19mer')\n",
    "sh_targets = sh_targets.rename(columns = {'Barcode Sequence': 'hp'}, inplace = False).set_index('hp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gene_name_df = sh_targets.ix[~sh_targets['Gene ID'].str.contains('^NO_CURRENT')].copy()\n",
    "# gene_name_df['Gene name'] = gene_name_df['Gene ID'] + ' (' + gene_name_df['Gene Symbol'] + ')'\n",
    "gene_name_df['Gene name'] = gene_name_df['Gene Symbol'] + ' (' + gene_name_df['Gene ID'] + ')'\n",
    "gene_name_map = gene_name_df.set_index('Gene ID', inplace = False)['Gene name']\n",
    "gene_name_map = gene_name_map.to_dict()\n",
    "\n",
    "gene_sym_map = gene_name_df.set_index('Gene ID', inplace = False)['Gene Symbol']\n",
    "gene_sym_map = gene_sym_map.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D2_description='''\n",
    "Results from DEMETER2 model fit.\n",
    "\n",
    "Contents:\n",
    "\n",
    "* gene_means_proc: posterior mean estimates of essentiality for each gene/CL pair\n",
    "* gene_SDs_proc: posterior SD of essentiality estimates for each gene/CL pair  \n",
    "* hp_data_comb: model results for each hairpin, including:\n",
    "    * Geff: hairpin gene knockdown efficacy [0,1]\n",
    "    * Seff: hairpin seed knockdown efficacy [0,1]\n",
    "    * unpred_offset_mean: posterior mean of across-CL avg unpredicted offtarget effect\n",
    "    * unpred_offset_sd: posterior SD of across-CL avg unpredicted offtarget effect\n",
    "* CL_data_comb: model results for each CL, including:\n",
    "    * gene_slope: RNAi efficacy parameter\n",
    "    * CL_slope: overall scaling factor\n",
    "    * noise_vars: noise variance\n",
    "    * offset_mean: posterior mean of additive offset\n",
    "    * offset_SD: posterior SD of additive offset\n",
    "    \n",
    "versions:\n",
    "* v2: \n",
    "    * Run with final hyperparameter settings\n",
    "    * Add gene symbols to entrez IDs\n",
    "    * exclude genes with all NA values\n",
    "    * exclude genes with poor reagents\n",
    "    * normalize gene scores to have median of pos-cons at -1 and median of neg-cons at 0\n",
    "\n",
    "* v3: \n",
    "    * Add gene symbols for gene families\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_processed_gene_data_D2(cur_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff):\n",
    "    '''Process gene means and SDs and make new files'''\n",
    "    gs = pd.read_csv(os.path.join(cur_model_dir, 'gene_means.csv'), index_col = 0)\n",
    "    gs_unc = pd.read_csv(os.path.join(cur_model_dir, 'gene_SDs.csv'), index_col = 0)\n",
    "    \n",
    "    #update cell line names\n",
    "    gs.columns = gs.columns.to_series().replace(new_name_map_dict)\n",
    "    gs_unc.columns = gs_unc.columns.to_series().replace(new_name_map_dict)\n",
    "    \n",
    "    #drop genes which are NA for all cell lines\n",
    "    bad_genes = np.where(gs.isnull().sum(axis = 1) == gs.shape[1])[0]\n",
    "    print('Removing {} genes with all NAs'.format(len(bad_genes)))\n",
    "    gs.drop(gs.index[bad_genes], axis=0, inplace=True)\n",
    "    gs_unc.drop(gs_unc.index[bad_genes], axis=0, inplace=True)\n",
    "\n",
    "    #calc mean Geff and sum Geff per gene, and filter out bad-quality genes\n",
    "    hp_data = pd.read_csv(os.path.join(cur_model_dir, 'hp_data.csv')).set_index('hp')\n",
    "    hp_data = hp_data.join(sh_targets, how = 'left')\n",
    "    hp_stats = hp_data.groupby('Gene ID').agg({'Geff': [np.mean, np.sum]})\n",
    "    bad_genes = hp_stats.ix[(hp_stats['Geff']['mean'].values < min_avg_Geff) | (hp_stats['Geff']['sum'].values < min_sum_Geff)].index.values\n",
    "    bad_genes = np.intersect1d(bad_genes, gs.index.values)\n",
    "    print('Removing {} genes with all poor hp data'.format(len(bad_genes)))\n",
    "    gs.drop(bad_genes, axis=0, inplace=True)\n",
    "    gs_unc.drop(bad_genes, axis=0, inplace=True)\n",
    "\n",
    "    #normalize gene scores by pos-neg control medians\n",
    "    weights = 1/(gs_unc**2)\n",
    "    per_gene_avgs = np.sum(gs * weights, axis = 1) / np.sum(weights, axis = 1)\n",
    "    pos_con_median = np.nanmedian(per_gene_avgs.ix[hart_ess])\n",
    "    neg_con_median = np.nanmedian(per_gene_avgs.ix[hart_non_ess])\n",
    "\n",
    "    norm_gs_unc = gs_unc / (neg_con_median - pos_con_median)\n",
    "    norm_gs = (gs - neg_con_median) / (neg_con_median - pos_con_median)\n",
    "\n",
    "    #rename genes to include gene symbol\n",
    "    norm_gs.rename(index = gene_name_map, inplace = True)\n",
    "    norm_gs_unc.rename(index = gene_name_map, inplace = True)\n",
    "    \n",
    "    #handle gene names for gene families\n",
    "    gene_families = np.where(norm_gs.index.str.contains('&'))[0]\n",
    "    ind_names = norm_gs.index.values\n",
    "    for fam_ind in gene_families:\n",
    "        cur_fam = norm_gs.index.values[fam_ind]\n",
    "        fam_syms = '&'.join([gene_sym_map[x] for x in re.split('&', cur_fam)])\n",
    "#         ind_names[fam_ind] = cur_fam + ' (' + fam_syms + ')'\n",
    "        ind_names[fam_ind] = fam_syms + ' (' + cur_fam + ')'\n",
    "    norm_gs.index = ind_names\n",
    "    norm_gs_unc.index = ind_names\n",
    "\n",
    "    norm_gs.to_csv(os.path.join(cur_model_dir, 'gene_means_proc.csv'))\n",
    "    norm_gs_unc.to_csv(os.path.join(cur_model_dir, 'gene_SDs_proc.csv'))\n",
    "\n",
    "    \n",
    "def prepare_D2_outputs(D2_model_dir):\n",
    "    '''Combine batch and non-batch parameters for CL data and hp_data'''\n",
    "    CL_data = pd.read_csv(os.path.join(D2_model_dir, 'CL_data.csv'), index_col = 0)\n",
    "        \n",
    "    CL_batch_data = pd.read_csv(os.path.join(D2_model_dir, 'CL_batch_data.csv'), index_col = 0)\n",
    "   \n",
    "    #rename cell lines\n",
    "    CL_data.index = CL_data.index.to_series().replace(new_name_map_dict)\n",
    "    CL_batch_data.index = CL_batch_data.index.to_series().replace(new_name_map_dict)\n",
    "    \n",
    "    CL_batch_data.reset_index(inplace=True)\n",
    "    CL_batch_data['offset_var'] = CL_batch_data['offset_sd']**2\n",
    "    CL_batch_means = CL_batch_data.groupby('CCLE_ID')[['CL_slope', 'noise_vars', 'offset_mean', 'offset_var']].agg('mean')\n",
    "    CL_batch_means['offset_sd'] = np.sqrt(CL_batch_means['offset_var'].values)\n",
    "\n",
    "    CL_data = pd.merge(CL_data, CL_batch_means[['CL_slope', 'noise_vars', 'offset_mean', 'offset_sd']], left_index=True, right_index = True)\n",
    "    CL_data.to_csv(os.path.join(D2_model_dir, 'CL_data_comb.csv'))\n",
    "    \n",
    "    hp_data = pd.read_csv(os.path.join(D2_model_dir, 'hp_data.csv')).set_index('hp')\n",
    "    hp_batch_data = pd.read_csv(os.path.join(D2_model_dir, 'hp_batch_data.csv')).reset_index()\n",
    "    hp_batch_data['hairpin_offset_var'] = hp_batch_data['hairpin_offset_sd']**2\n",
    "    hp_batch_means = hp_batch_data.groupby('hp')[['hairpin_offset_mean', 'hairpin_offset_var']].agg('mean')\n",
    "    hp_batch_means['hairpin_offset_sd'] = np.sqrt(hp_batch_means['hairpin_offset_var'].values)\n",
    "\n",
    "    hp_data = pd.merge(hp_data[['Geff', 'Seff', 'unpred_offset_mean', 'unpred_offset_sd']], hp_batch_means[['hairpin_offset_sd', 'hairpin_offset_mean']], left_index=True, right_index = True)\n",
    "    hp_data.to_csv(os.path.join(D2_model_dir, 'hp_data_comb.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Achilles data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 520 genes with all NAs\n",
      "Removing 119 genes with all poor hp data\n",
      "Now choosing the datasets you would want to keep or remove:\n",
      "\tKeep hp_data_comb ? (y/n) n\n",
      "\tNot keeping hp_data_comb\n",
      "\tKeep gene_effect ? (y/n) n\n",
      "\tNot keeping gene_effect\n",
      "\tKeep pan_dependent_genes ? (y/n) y\n",
      "\tKeep gene_fdr ? (y/n) n\n",
      "\tNot keeping gene_fdr\n",
      "\tKeep sample_info ? (y/n) y\n",
      "\tKeep gene_dependency ? (y/n) n\n",
      "\tNot keeping gene_dependency\n",
      "\tKeep gene_SDs_proc ? (y/n) n\n",
      "\tNot keeping gene_SDs_proc\n",
      "\tKeep CL_data_comb ? (y/n) n\n",
      "\tNot keeping CL_data_comb\n",
      "\tKeep gene_means_proc ? (y/n) n\n",
      "\tNot keeping gene_means_proc\n",
      "Uploading CL_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Waiting in the task queue\n",
      "\n",
      "\t Done: CL_data_comb properly converted and uploaded\n",
      "Uploading hp_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 93750\n",
      "\n",
      "\t Done: hp_data_comb properly converted and uploaded\n",
      "Uploading gene_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 9001)\n",
      "\t Conversion in progress, line 3500\n",
      "\t Conversion in progress, line 12000\n",
      "\t Conversion in progress, line 12250\n",
      "\t Conversion in progress, line 12250\n",
      "\t Conversion in progress, line 17000\n",
      "\t Conversion in progress, line 17000\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: gene_SDs_proc properly converted and uploaded\n",
      "Uploading gene_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 7001)\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 9000\n",
      "\t Conversion in progress, line 12250\n",
      "\t Conversion in progress, line 12250\n",
      "\t Conversion in progress, line 13000\n",
      "\t Conversion in progress, line 17000\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: gene_means_proc properly converted and uploaded\n",
      "Creating the new version with these files:\n",
      "\tNEW: CL_data_comb - NumericMatrixCSV\n",
      "\tNEW: hp_data_comb - NumericMatrixCSV\n",
      "\tNEW: gene_SDs_proc - NumericMatrixCSV\n",
      "\tNEW: gene_means_proc - NumericMatrixCSV\n",
      "\tKEEP: pan_dependent_genes - Columnar\n",
      "\tKEEP: sample_info - Columnar\n",
      "\n",
      "Dataset version with id efd1d6ee7fbd47679fba530eb387070f created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/efd1d6ee7fbd47679fba530eb387070f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'efd1d6ee7fbd47679fba530eb387070f'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_processed_gene_data_D2(Ach_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(Ach_model_dir)\n",
    "\n",
    "c.update_dataset(dataset_permaname=Ach_dataset_id,\n",
    "#     dataset_description=D2_description,\n",
    "    upload_file_path_dict={os.path.join(Ach_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Ach_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Ach_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Ach_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRIVE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 725 genes with all NAs\n",
      "Removing 287 genes with all poor hp data\n",
      "Now choosing the datasets you would want to keep or remove:\n",
      "\tKeep CL_data_comb ? (y/n) n\n",
      "\tNot keeping CL_data_comb\n",
      "\tKeep gene_means_proc ? (y/n) n\n",
      "\tNot keeping gene_means_proc\n",
      "\tKeep gene_SDs_proc ? (y/n) n\n",
      "\tNot keeping gene_SDs_proc\n",
      "\tKeep hp_data_comb ? (y/n) n\n",
      "\tNot keeping hp_data_comb\n",
      "\tKeep pan_dependent_genes ? (y/n) y\n",
      "Uploading gene_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 6001)\n",
      "\t Conversion in progress, line 8250\n",
      "\t Conversion in progress, line 8250\n",
      "\n",
      "\t Done: gene_means_proc properly converted and uploaded\n",
      "Uploading hp_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 157500\n",
      "\n",
      "\t Done: hp_data_comb properly converted and uploaded\n",
      "Uploading CL_data_comb...\n",
      "Conversion and upload...:\n",
      "\n",
      "\t Done: CL_data_comb properly converted and uploaded\n",
      "Uploading gene_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 4001)\n",
      "\t Conversion in progress, line 7750\n",
      "\t Conversion in progress, line 8250\n",
      "\n",
      "\t Done: gene_SDs_proc properly converted and uploaded\n",
      "Creating the new version with these files:\n",
      "\tNEW: gene_means_proc - NumericMatrixCSV\n",
      "\tNEW: hp_data_comb - NumericMatrixCSV\n",
      "\tNEW: CL_data_comb - NumericMatrixCSV\n",
      "\tNEW: gene_SDs_proc - NumericMatrixCSV\n",
      "\tKEEP: pan_dependent_genes - Columnar\n",
      "\n",
      "Dataset version with id ed9ab0174ce84677ae1a91933e953251 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/ed9ab0174ce84677ae1a91933e953251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'ed9ab0174ce84677ae1a91933e953251'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_processed_gene_data_D2(DRIVE_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(DRIVE_model_dir)\n",
    "\n",
    "c.update_dataset(dataset_permaname=DRIVE_dataset_id,\n",
    "#     dataset_description=D2_description,\n",
    "    upload_file_path_dict={os.path.join(DRIVE_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(DRIVE_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(DRIVE_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(DRIVE_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcotte data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 703 genes with all NAs\n",
      "Removing 2620 genes with all poor hp data\n",
      "Now choosing the datasets you would want to keep or remove:\n",
      "\tKeep CL_data_comb ? (y/n) n\n",
      "\tNot keeping CL_data_comb\n",
      "\tKeep gene_means_proc ? (y/n) n\n",
      "\tNot keeping gene_means_proc\n",
      "\tKeep gene_SDs_proc ? (y/n) n\n",
      "\tNot keeping gene_SDs_proc\n",
      "\tKeep hp_data_comb ? (y/n) n\n",
      "\tNot keeping hp_data_comb\n",
      "Uploading gene_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 13000\n",
      "\n",
      "\t Done: gene_means_proc properly converted and uploaded\n",
      "Uploading CL_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: CL_data_comb properly converted and uploaded\n",
      "Uploading hp_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 82250\n",
      "\n",
      "\t Done: hp_data_comb properly converted and uploaded\n",
      "Uploading gene_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Conversion in progress, line 13000\n",
      "\n",
      "\t Done: gene_SDs_proc properly converted and uploaded\n",
      "Creating the new version with these files:\n",
      "\tNEW: gene_means_proc - NumericMatrixCSV\n",
      "\tNEW: CL_data_comb - NumericMatrixCSV\n",
      "\tNEW: hp_data_comb - NumericMatrixCSV\n",
      "\tNEW: gene_SDs_proc - NumericMatrixCSV\n",
      "\n",
      "Dataset version with id 17ca587ec68a44948296d660454f1496 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/17ca587ec68a44948296d660454f1496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'17ca587ec68a44948296d660454f1496'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_processed_gene_data_D2(Marc_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(Marc_model_dir)\n",
    "\n",
    "c.update_dataset(dataset_permaname=Marc_dataset_id,\n",
    "#     dataset_description=D2_description,\n",
    "    upload_file_path_dict={os.path.join(Marc_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Marc_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Marc_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(Marc_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 513 genes with all NAs\n",
      "Removing 802 genes with all poor hp data\n",
      "Now choosing the datasets you would want to keep or remove:\n",
      "\tKeep CL_data_comb ? (y/n) n\n",
      "\tNot keeping CL_data_comb\n",
      "\tKeep gene_means_proc ? (y/n) n\n",
      "\tNot keeping gene_means_proc\n",
      "\tKeep gene_SDs_proc ? (y/n) n\n",
      "\tNot keeping gene_SDs_proc\n",
      "\tKeep hp_data_comb ? (y/n) n\n",
      "\tNot keeping hp_data_comb\n",
      "\tKeep pan_dependent_genes ? (y/n) y\n",
      "Uploading gene_means_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 7001)\n",
      "\t Scanning through file to determine size (line 15001)\n",
      "\t Conversion in progress, line 7250\n",
      "\t Conversion in progress, line 8750\n",
      "\t Conversion in progress, line 9250\n",
      "\t Conversion in progress, line 16500\n",
      "\t Conversion in progress, line 17500\n",
      "\t Conversion in progress, line 17500\n",
      "\t Conversion in progress, line 17500\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: gene_means_proc properly converted and uploaded\n",
      "Uploading hp_data_comb...\n",
      "Conversion and upload...:\n",
      "\t Conversion in progress, line 34250\n",
      "\t Conversion in progress, line 240250\n",
      "\t Conversion in progress, line 240250\n",
      "\n",
      "\t Done: hp_data_comb properly converted and uploaded\n",
      "Uploading CL_data_comb...\n",
      "Conversion and upload...:\n",
      "\n",
      "\t Done: CL_data_comb properly converted and uploaded\n",
      "Uploading gene_SDs_proc...\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 2001)\n",
      "\t Scanning through file to determine size (line 11001)\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 8750\n",
      "\t Conversion in progress, line 8750\n",
      "\t Conversion in progress, line 8750\n",
      "\t Conversion in progress, line 14000\n",
      "\t Conversion in progress, line 17500\n",
      "\t Conversion in progress, line 17500\n",
      "\t Conversion in progress, line 17500\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: gene_SDs_proc properly converted and uploaded\n",
      "Creating the new version with these files:\n",
      "\tNEW: gene_means_proc - NumericMatrixCSV\n",
      "\tNEW: hp_data_comb - NumericMatrixCSV\n",
      "\tNEW: CL_data_comb - NumericMatrixCSV\n",
      "\tNEW: gene_SDs_proc - NumericMatrixCSV\n",
      "\tKEEP: pan_dependent_genes - Columnar\n",
      "\n",
      "Dataset version with id f0b624a8eabc42a4bfd37a7a4c945004 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/f0b624a8eabc42a4bfd37a7a4c945004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'f0b624a8eabc42a4bfd37a7a4c945004'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_processed_gene_data_D2(comb_model_dir, gene_name_map, min_avg_Geff, min_sum_Geff)\n",
    "prepare_D2_outputs(comb_model_dir)\n",
    "\n",
    "c.update_dataset(dataset_permaname=comb_dataset_id,\n",
    "#     dataset_description=D2_description,\n",
    "    upload_file_path_dict={os.path.join(comb_model_dir, 'CL_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(comb_model_dir, 'hp_data_comb.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(comb_model_dir, 'gene_means_proc.csv'): 'NumericMatrixCSV',\n",
    "                          os.path.join(comb_model_dir, 'gene_SDs_proc.csv'): 'NumericMatrixCSV'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
